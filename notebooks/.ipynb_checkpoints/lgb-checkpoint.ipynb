{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55f0ed2ac468f0eb59c24726ca0c9bdc8884c345"
   },
   "source": [
    "This model train LightGBMRegressor and dump results (as second level feature) to file for later ensembling. \n",
    "\n",
    "validation strategy: month=34 test, month=33 validation, month 12-32 train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f03379ee467570732ebb2b3d20062fea0584d57d"
   },
   "source": [
    "# Exploratary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0a1c729d4fb3d6609f9dfb163ebe92fa9dc654c"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "items = pd.read_csv('../input/items.csv')\n",
    "shops = pd.read_csv('../input/shops.csv')\n",
    "cats = pd.read_csv('../input/item_categories.csv')\n",
    "train = pd.read_csv('../input/sales_train.csv')\n",
    "# set index to ID to avoid droping it later\n",
    "test  = pd.read_csv('../input/test.csv').set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4441f3aa0223a0c82dc0857668f2b66b51796617"
   },
   "source": [
    "Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bffdf025a7d56c073baae2be28dc020ea970c705"
   },
   "outputs": [],
   "source": [
    "print('train size, item in train, shop in train', train.shape[0], train.item_id.nunique(), train.shop_id.nunique())\n",
    "print('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\n",
    "print('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56159823a46bb6dad909a3e1e86c1b7eafd3c54e"
   },
   "source": [
    "The total sale of each month, we see a clear trend and seasonality. The overall sale is decreasing with time, and there are peaks in November."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8da35b420ce7058eeb91d44378317d233759436a"
   },
   "outputs": [],
   "source": [
    "sale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\n",
    "sale_by_month.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafc3711b504497b24f60e1fc1038b961e996ad4"
   },
   "source": [
    "The distribution of sale grouped by month, item and shop, we see most item-shop pairs have small monthly sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8bc02e1ca222b8ce670cf53e3697bc7cb3d9566"
   },
   "outputs": [],
   "source": [
    "block_item_shop_sale = train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\n",
    "block_item_shop_sale.clip(0,20).plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed7a190645750a818e29a6291ba2553a91764c7c"
   },
   "source": [
    "\n",
    "# Feature engineering and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "425d8f2dc08378977b393bf80c5fdcf0fba2c992"
   },
   "source": [
    "#### remove outliers\n",
    "There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a864412fafc3129a3e9bd5bb1f18a7cf0c62935"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=train.item_cnt_day)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(train.item_price.min(), train.item_price.max()*1.1)\n",
    "sns.boxplot(x=train.item_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e621535d112603c60aeb2c2f83dbbf96d36b732"
   },
   "outputs": [],
   "source": [
    "train = train[train.item_price<100000]\n",
    "train = train[train.item_cnt_day<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2f99368478e3063b1c379537944e954d7186928"
   },
   "source": [
    "There is one item with price below zero. Fill it with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fc6b90b22fe232f4240ac8f965cc52b3db5526a"
   },
   "outputs": [],
   "source": [
    "median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\n",
    "train.loc[train.item_price<0, 'item_price'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7da194c285d696b5c6978148bf0143b9b2a7b0c5"
   },
   "source": [
    "Several shops are duplicates of each other (according to its name). Fix train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00fe91e9c482ea413abd774ff903fe3d152785dd"
   },
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a30f0521464e1fa20444e66d24bbdcb76b93f6de"
   },
   "source": [
    "#### Shops/Cats/Items preprocessing\n",
    "Observations:\n",
    "* Each shop_name starts with the city name.\n",
    "* Each category contains type and subtype in its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12fae4c8d0c8f3e817307d1e0ffc6831e9a8d696"
   },
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "shops = shops[['shop_id','city_code']]\n",
    "\n",
    "cats['split'] = cats['item_category_name'].str.split('-')\n",
    "cats['type'] = cats['split'].map(lambda x: x[0].strip())\n",
    "cats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n",
    "# if subtype is nan then type\n",
    "cats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "cats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n",
    "cats = cats[['item_category_id','type_code', 'subtype_code']]\n",
    "\n",
    "items.drop(['item_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62c5f83fa222595da99294f465ab28e80ce415e9"
   },
   "source": [
    "## Monthly sales\n",
    "Most of the items in the test set target value should be zero, while train set contains only pairs which were sold or returned in the past. So we expand the train set to include those item-shop pairs with zero monthly sales. This way train data will be similar to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7626c7455ea71b65894c6c866519df15080fa2ac"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = train[train.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "    \n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix['item_id'] = matrix['item_id'].astype(np.int16)\n",
    "matrix.sort_values(cols,inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867e91a7570dd78b4834f4f1a166e58f80b63f93"
   },
   "source": [
    "Aggregate train set by shop/item pairs to calculate target aggreagates, then <b>clip(0,20)</b> target value. This way train target will be similar to the test predictions.\n",
    "\n",
    "Downcast item_cnt_month to float32 -- float16 was too small to perform sum operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fef5477060be7d2e6c85dcb79d8e18e6253f7dd"
   },
   "outputs": [],
   "source": [
    "train['revenue'] = train['item_price'] *  train['item_cnt_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dd27181918fc7df89676e24d72130d183929d2d"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n",
    "group.columns = ['item_cnt_month']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=cols, how='left')\n",
    "matrix['item_cnt_month'] = (matrix['item_cnt_month']\n",
    "                                .fillna(0)\n",
    "                                .clip(0,20) # NB clip target here\n",
    "                                .astype(np.float32))\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "315bc6107a93f3926a64fd09ea9244e9281ee41f"
   },
   "source": [
    "## Test set\n",
    "To use time tricks append test pairs to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29d02bdb4fa768577607bf735b918ca81da85d41"
   },
   "outputs": [],
   "source": [
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "177fbbab94c8057d67d61357d29581248468a74d"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\n",
    "matrix.fillna(0, inplace=True) # 34 month\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "233e394a6cebf36ef002dc76fef8d430026a52b3"
   },
   "source": [
    "## Shops/Items/Cats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dfd5df3e2bcaee4c312f3979736f52c40f2560f"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n",
    "matrix = pd.merge(matrix, items, on=['item_id'], how='left')\n",
    "matrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\n",
    "matrix['city_code'] = matrix['city_code'].astype(np.int8)\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['type_code'] = matrix['type_code'].astype(np.int8)\n",
    "matrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8358b291fdc8e0e7d1b5700974803b3f104715f7"
   },
   "source": [
    "## Traget lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cd7bcc7643ce4545475e8e6f80d09a979aac42d"
   },
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78bf7ece93ebc4629ad0e48cd6a9927788d8706d"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9623850fd14c1a67cb8d68f536b32d974729032b"
   },
   "source": [
    "## Group sale stats in recent\n",
    "create stats (mean/var) of sales of certain groups during the past 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46eb3502a9c5cc2dbd179613f09cc587d17128f5"
   },
   "outputs": [],
   "source": [
    "def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n",
    "    if not 'date_block_num' in groupby_feats:\n",
    "        print ('date_block_num must in groupby_feats')\n",
    "        return matrix_\n",
    "    \n",
    "    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n",
    "    max_lags = np.max(last_periods)\n",
    "    for i in range(1,max_lags+1):\n",
    "        shifted = group[groupby_feats+[target]].copy(deep=True)\n",
    "        shifted['date_block_num'] += i\n",
    "        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n",
    "        group = group.merge(shifted, on=groupby_feats, how='left')\n",
    "    group.fillna(0,inplace=True)\n",
    "    for period in last_periods:\n",
    "        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n",
    "        # we do not use mean and svd directly because we want to include months with sales = 0\n",
    "        mean = group[lag_feats].sum(axis=1)/float(period)\n",
    "        mean2 = (group[lag_feats]**2).sum(axis=1)/float(period)\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n",
    "        # divide by mean, this scales the features for NN\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] /= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] /= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n",
    "    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n",
    "    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "041621b2e2e5d28f91bfc6ef58dea9b0a65a08d9"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c67bf4dbcef884ffe9d19c65d37bc4de1f287ef6"
   },
   "source": [
    "## Mean encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79c8e250e42eaabddc4bc6c6569e70e1a23e2cfe"
   },
   "outputs": [],
   "source": [
    "def target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n",
    "    print ('target encoding for',groupby_feats)\n",
    "    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n",
    "    group.columns = [enc_feat]\n",
    "    group.reset_index(inplace=True)\n",
    "    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n",
    "    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n",
    "    matrix = lag_feature(matrix, lags, enc_feat)\n",
    "    matrix.drop(enc_feat, axis=1, inplace=True)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56d7d2b826231476ce450e90a9c8d9e68ed471e5"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bcea31d93ab035ca3fa1ed7c0afddbf602c414a"
   },
   "source": [
    "## Trend features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0504e9613087237c255914d9ebd165fac4e88cd0"
   },
   "source": [
    "Price trend for the last six months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0da2ded8502e273137991fd2bebbadaf19c19622"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n",
    "# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n",
    "# Invalid dtype for backfill_2d [float16]\n",
    "\n",
    "fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    fetures_to_drop += ['delta_price_lag_'+str(i)]\n",
    "\n",
    "matrix.drop(fetures_to_drop, axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17765ddb48f52abd88847a42c0a3ffe974e5b121"
   },
   "source": [
    "Last month shop revenue trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e633be47f1a22b41487866ce67fb874bd296339e"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n",
    "group.columns = ['date_shop_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n",
    "group.columns = ['shop_avg_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['shop_id'], how='left')\n",
    "matrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n",
    "\n",
    "matrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], 'delta_revenue')\n",
    "\n",
    "matrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47e06af411b7d26cd93dad3d6735e48e5fbdee50"
   },
   "source": [
    "## Add month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb521e1f33d4124a3b90b47447bdb29150770b6e"
   },
   "outputs": [],
   "source": [
    "matrix['month'] = matrix['date_block_num'] % 12\n",
    "matrix['year'] = (matrix['date_block_num'] / 12).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c096e86eb0043c0f6eeb899de24e28ca4c4e044"
   },
   "source": [
    "Months since the last sale for each shop/item pair and for item only. I use programing approach.\n",
    "\n",
    "<i>Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3458a7056c963167760921417d1f863f074f2b39"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_shop_last_sale'] = -1\n",
    "matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = str(row.item_id)+' '+str(row.shop_id)\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n",
    "        cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28b29fae3906d870b4dc3064a7f359b6d3abf623"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_last_sale'] = -1\n",
    "matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = row.item_id\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        if row.date_block_num>last_date_block_num:\n",
    "            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n",
    "            cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61987e6adc1bec2ea897eec837c0253f7f73fdb5"
   },
   "source": [
    "Months since the first sale for each shop/item pair and for item only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad0869709bbada35726d5ca41dd913d817249f8e"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "966cb34ccfe849fbb3707d93270691cb8eef7a89"
   },
   "source": [
    "## Final preparations\n",
    "Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).\n",
    "Lightgbm and XGBboost can deal with missing values, so we will leave the NaNs as it is. Later for neural network, we will fill na with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00bf3fffc1b143d0555d03b9d79b5fd00d9d0dc9"
   },
   "outputs": [],
   "source": [
    "matrix = matrix[matrix.date_block_num > 11]\n",
    "matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d9988f8da8876f74092fbf827ceb6c61dd09d5e"
   },
   "outputs": [],
   "source": [
    "matrix.to_pickle('data.pkl')\n",
    "del matrix\n",
    "del cache\n",
    "del group\n",
    "del items\n",
    "del shops\n",
    "del cats\n",
    "del train\n",
    "# leave test for submission\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b69932efb440af8f6435f3cd802fbcd15682af71",
    "collapsed": true
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a54364495b1818e9f069efa0c53500bf9e21d5f9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')\n",
    "data = data[[\n",
    "    'date_block_num',\n",
    "    'shop_id',\n",
    "    #'item_id',\n",
    "    'item_cnt_month',\n",
    "    'city_code',\n",
    "    'item_category_id',\n",
    "    'type_code','subtype_code',\n",
    "    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n",
    "    'item_avg_sale_last_6', 'item_std_sale_last_6',\n",
    "    'item_avg_sale_last_12', 'item_std_sale_last_12',\n",
    "    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n",
    "    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n",
    "    'category_avg_sale_last_12', 'category_std_sale_last_12',\n",
    "    'city_avg_sale_last_12', 'city_std_sale_last_12',\n",
    "    'type_avg_sale_last_12', 'type_std_sale_last_12',\n",
    "    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n",
    "    'date_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n",
    "    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n",
    "    'date_cat_avg_item_cnt_lag_1',\n",
    "    'date_shop_cat_avg_item_cnt_lag_1',\n",
    "    'date_city_avg_item_cnt_lag_1',\n",
    "    'date_item_city_avg_item_cnt_lag_1',\n",
    "    'delta_price_lag',\n",
    "    'month','year',\n",
    "    'item_shop_last_sale','item_last_sale',\n",
    "    'item_shop_first_sale','item_first_sale',\n",
    "]]\n",
    "\n",
    "cat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11eb4f2f5ada18aa8993ec55e8c63e80758fc19e"
   },
   "source": [
    "Validation strategy is 34 month for the test set, 33 month for the validation set and 13-32 months for the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9af76d7b80064573a453e5e10c35b76fc31c47a4"
   },
   "outputs": [],
   "source": [
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n",
    "\n",
    "del data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b37f26889499cbd5df4e44d235e35491d00b09cb"
   },
   "source": [
    "### Create space for second level training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ea5437e8949db6d3e54e68b7b0c18cd0befe38c"
   },
   "outputs": [],
   "source": [
    "X_train_level2 = pd.DataFrame(np.zeros([X_valid.shape[0],0]))\n",
    "Y_train_level2 = Y_valid\n",
    "X_test_level2 = pd.DataFrame(np.zeros([X_test.shape[0],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65da4e1e31a61cef33589b4a7cf3205137ad9f39"
   },
   "source": [
    "### First model LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acef75c36501f808d45f81fc69f9708fc3283bc3"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    max_depth = 8,\n",
    "    n_estimators = 500,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight = 300,\n",
    "    reg_alpha = 0.1,\n",
    "    reg_lambda = 1,\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=10, \n",
    "    early_stopping_rounds = 40,\n",
    "    categorical_feature = cat_feats) # use LGBM's build-in categroical features.\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ff5a80a22d046c5ca1cb27e938c757b607551d2"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "\n",
    "X_train_level2['lgb'] = Y_pred\n",
    "X_test_level2['lgb'] = Y_test\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('lgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0aa93ac5b148fba5896a4147530e39b8c4c8c0f6"
   },
   "source": [
    "### second model XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "468264b65a5038448d3f3e5ec25b52b567896718"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "# drop cat_feats since XGBoost and NN cannot deal with label encoded categorical features\n",
    "X_train.drop(cat_feats, inplace=True, axis=1)\n",
    "X_valid.drop(cat_feats, inplace=True, axis=1)\n",
    "X_test.drop(cat_feats, inplace=True, axis=1)\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=7,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    gamma = 0.005,\n",
    "    eta=0.1,    \n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=10, \n",
    "    early_stopping_rounds = 40,\n",
    "    )\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8adc7c93323eb77baeceb2e8db17390b5c4deb3"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "\n",
    "X_train_level2['xgb'] = Y_pred\n",
    "X_test_level2['xgb'] = Y_test\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('xgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f423d9c797980351c1cbc0fbe9cfe88ff1ae37c"
   },
   "source": [
    "### Third model, multilayer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a640357b501e2dd8bc3194ee4fa0b7dd69ff1f9b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(23333)\n",
    "np.random.seed(233333)\n",
    "\n",
    "# define model\n",
    "def Sales_prediction_model(input_shape):\n",
    "    in_layer = Input(input_shape)\n",
    "    x = Dense(16,kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(in_layer)\n",
    "    x = Dense(8, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n",
    "    x = Dense(1, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n",
    "    \n",
    "    model = Model(inputs = in_layer, outputs = x, name='Sales_prediction_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f6e9cb894785d955c58fdd525a1e6dced6636bd"
   },
   "outputs": [],
   "source": [
    "# preprocessing data for NN\n",
    "X_train_nn = X_train.copy(deep=True)\n",
    "X_valid_nn = X_valid.copy(deep=True)\n",
    "X_test_nn = X_test.copy(deep=True)\n",
    "\n",
    "X_train_nn.fillna(0,inplace=True)\n",
    "X_valid_nn.fillna(0,inplace=True)\n",
    "X_test_nn.fillna(0,inplace=True)\n",
    "\n",
    "input_shape = [X_train_nn.shape[1]]\n",
    "model = Sales_prediction_model(input_shape)\n",
    "model.compile(optimizer = Adam(lr=0.0005) , loss = [\"mse\"], metrics=['mse'])\n",
    "model.fit(X_train_nn, Y_train, validation_data = (X_valid_nn, Y_valid), batch_size = 10000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd4bb171512bb53c9d610a0b0ce2a3928b121074"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid_nn).clip(0, 20)\n",
    "Y_test = model.predict(X_test_nn).clip(0, 20)[:,0]\n",
    "\n",
    "X_train_level2['nn'] = Y_pred\n",
    "X_test_level2['nn'] = Y_test\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('nn_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a67f292ee40f6269fee86bc17a01f640c35e7c68"
   },
   "source": [
    "### Build second level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca80ea21cc9328b92acc49b735e3c6efc2c7b4bf"
   },
   "outputs": [],
   "source": [
    "for col in X_train_level2.columns:\n",
    "    rmse = np.sqrt(mean_squared_error(Y_train_level2, X_train_level2[col]))\n",
    "    print('model',col,' rmse: ',rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9a893ea58cdef774c250bff83151f83db235cd5"
   },
   "outputs": [],
   "source": [
    "#simple weighted average\n",
    "Y_pred_level2 = 0.8*X_train_level2['lgb'] + 0.2*X_train_level2['xgb']\n",
    "Y_test_level2 = 0.8*X_test_level2['lgb'] + 0.2*X_test_level2['xgb']\n",
    "rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n",
    "print('simple average of lgb and xgb validation rmse: ',rmse)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test_level2\n",
    "})\n",
    "submission.to_csv('blended_submission1.csv', index=False)\n",
    "\n",
    "#Linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_level2, Y_train_level2)\n",
    "Y_pred_level2 = model.predict(X_train_level2)\n",
    "Y_test_level2 = model.predict(X_test_level2)\n",
    "rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n",
    "print('Linear regression validation rmse: ',rmse)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test_level2\n",
    "})\n",
    "submission.to_csv('blended_submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
